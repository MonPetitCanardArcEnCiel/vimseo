{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nManage model results\n====================\n\nManage model results using several result managers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport logging\n\nimport mlflow\nfrom gemseo.datasets.dataset import Dataset\nfrom gemseo.post.dataset.bars import BarPlot\nfrom mlflow import delete_run\nfrom numpy import atleast_1d\nfrom pandas import DataFrame\nfrom pandas import concat\n\nfrom vimseo import EXAMPLE_RUNS_DIR_NAME\nfrom vimseo.api import activate_logger\nfrom vimseo.api import create_model\nfrom vimseo.core.model_result import ModelResult\nfrom vimseo.core.model_settings import IntegratedModelSettings\nfrom vimseo.storage_management.base_storage_manager import PersistencyPolicy\nfrom vimseo.utilities.plotting_utils import plot_curves\n\nactivate_logger(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Properly storing model results when running hundreds of simulations with possibly several users and several studies\nat the same time is a key aspect, especially if model credibility demonstration is targeted.\nOur strategy is to separate the following concepts:\n - scratch results: these are specific to models relying on external solvers. A scratch directory contains the data\n   read or written by the solver.\n - archive results: these are the data we want to keep after the simulation. They comprise the input and output data,\n   metadata and files generated in the scratch directory that we want to conserve.\n - a cache: while a cache could be seen as a way of archiving results, we prefer to separate cache and archiving.\n   However, a bridge is available between both: a cache can be generated from an archive or part of an archive.\n\nAn archive manager can be chosen among ``DirectoryArchive`` or ``MlflowArchive``.\nIn the first case, an arborescence of directories is generated under a directory ``default_archive``\ncreated in the current working directory.\nIn the second case, an MLflow database is created.\nBy default the database is file-based and is created under a directory ``mlflow_archive``\nin the current working directory.\nThe root directory of the archive can be specified with argument ``directory_archive_root``\npassed to the constructor of the model or to ``create_model()``.\n\nThe scratch results are stored in an arborescence of directories similarly to a ``DirectoryArchive``.\n\nThe persistency policy of the scratch and the archive can be specified independently.\n\nBy default, the storage generates unique directories at each new model execution.\nFor a ``DirectoryArchive``, the argument ``job_name`` allows to store the result in a specific directory\nwithout creating unique directories.\n\nHere, a model is created with the default ``DirectoryArchive`` manager, and a specific job directory is used to\nstore the archive result. The ``_accept_overwrite_job_dir`` attribute must be explicitly set to ``True``.\nThe ``MockModelPersistent`` model has the specificity of requiring to store some generated files to the archive.\nThe archive manager automatically handles the copy of these files from the scratch to the archive.\nHere, the scratch directories are kept such that the user can look into them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"MockModelPersistent\"\nload_case = \"LC1\"\nmodel = create_model(\n    model_name,\n    load_case,\n    IntegratedModelSettings(\n        directory_scratch_persistency=PersistencyPolicy.DELETE_NEVER,\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/archive/visualize_model_result\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/visualize_model_result\",\n        cache_file_path=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/caches/visualize_model_result/{model_name}_{load_case}_cache.hdf\",\n    ),\n)\nmodel.archive_manager._accept_overwrite_job_dir = True\nmodel.cache = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An execution is then performed. A ``JSON`` file is written in the directory ``my_experiment`` of the archive.\nIt contains the input data, output data and metadata.\nTwo files identified at model-level as being persistent have also been copied from the scratch to the archive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.execute({\"x1\": atleast_1d(2.0), \"x2\": atleast_1d(-2.0)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It often occurs that a cache becomes unusable due to:\n - a change of VIMSEO version, altering the input, output or metadata of a model,\n - a mix of successful and failed simulations (by fail, we mean that models may be fault tolerant and\n   outputs NaNs in the output data in case of errors). In general, we want to filter out failed runs.\n - a large number of results, and for part of them, we have no traceability about and we do not trust the\n   results.\nThen, a useful feature is to generate a cache file from an archive.\n``create_cache_from_archive()`` generates a cache file with same name as the current cache suffixed by '_from_archive'.\nFor a ``DirectoryArchive``, the whole archive is considered (no filtering possible):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_from_archive = model.create_cache_from_archive()\nprint(cache_from_archive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An archive storage based on [MLflow](https://mlflow.org) is also available.\nMLflow stores the runs by so-called 'Experiments'.\nThe default experiment name is ``{model_name}_{load_case}``.\nAgain, we create a model instance:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"BendingTestAnalytical\"\nload_case = \"Cantilever\"\nmodel = create_model(\n    model_name,\n    load_case,\n    model_options=IntegratedModelSettings(\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/mlflow_archive/visualize_model_result\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/visualize_model_result\",\n        cache_file_path=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/caches/visualize_model_result/{model_name}_{load_case}_cache.hdf\",\n        archive_manager=\"MlflowArchive\",\n    ),\n)\nmodel.cache = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All runs for the default and specific (specified below as ``my_experiment``) experiment name\nare first deleted:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "runs = mlflow.search_runs(\n    experiment_names=[\"BendingTestAnalytical_Cantilever\", \"my_experiment\"]\n)\nfor id in runs[\"run_id\"]:\n    delete_run(id)\n\n# A first execution is done. Its result is stored under the default experiment name,\n# which is ``{model_name}_{load_case}``:\nmodel.execute({\"height\": atleast_1d(40.0)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, another experiment is defined.\nIt can be useful to group runs for a given study.\nThe model is then executed for two different beam height:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.archive_manager.set_experiment(\"my_experiment\")\nmodel.execute({\"height\": atleast_1d(50.0)})\nmodel.execute({\"height\": atleast_1d(60.0)})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The archive results can be visualized through the MLflow User Interface.\nThe ``uri`` of the MLflow database must be specified to the UI:\n - For exemple, Under Windows:\n   ``mlflow ui --backend-store-uri file:\\\\\\\\\\\\C:\\\\Users\\\\sebastien.bocquet\\\\PycharmProjects\\\\vimseo\\\\tests\\\\storage_management\\\\my_experiment``\n - Or under Linux:\n   ``mlflow ui --backend-store-uri file:////home/sebastien.bocquet/PycharmProjects/vims_only/doc_src/_examples/02-integrated_models/mlflow_archive``\nThe uri can be retrieved with ``model._storage_manager.uri``.\n\nThe mapping from model raw results (input, output, metadata) and MLflow result tracking framework is done as follows:\n - all numbers among the input and output data (including arrays of size one, from which the number is extracted),\n   are stored as MLflow ``metrics``. Inputs are distinguished by prefixing their name by ``inputs.``,\n - all other type of data among the input and output data are considered as MLflow ``params``. They are jsonified\n   and stored as strings,\n - the metadata are stored as MLflow ``tags``.\nThe UI segregates the runs by experiment, and provides a\n[searching functionality](https://mlflow.org/docs/latest/ml/search/search-runs/).\nMLflow is compatible with several databases (remote PostgreSql, Amazon S3 etc...),\nallowing the archive storage to scale (company level or project level).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Runs can also be searched programmatically, here by experiment and for a given input range.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "runs = mlflow.search_runs(\n    experiment_names=[\"my_experiment\"],\n    filter_string=\"metrics.inputs.height > 50.0\",\n)\nassert len(runs) == 1\nruns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Metadata could also be used in the query, for instance ``tags.user = \"a_user\"``.\nNote that the result returned by ``get_result()`` has the following format:\n``{\"inputs\": input_data, \"output\": output_data}``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "archive_result = model.archive_manager.get_result(runs[\"run_id\"][0])\narchive_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be converted to a user-friendly result format [ModelResult][vimseo.core.model_result.ModelResult]\nthat can be easily visualized and compared:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result = ModelResult.from_data(archive_result)\nresult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform another query to retrieve the second run:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "runs = mlflow.search_runs(\n    experiment_names=[\"my_experiment\"],\n    filter_string=\"metrics.inputs.height <= 51.0 AND metrics.inputs.height >= 49.0\",\n)\nassert len(runs) == 1\narchive_result_1 = model.archive_manager.get_result(runs[\"run_id\"][0])\nresult_1 = ModelResult.from_data(archive_result_1)\nresult_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to create a cache file from the archive.\nBy default, the runs of the current experiment are considered.\nSince experiment \"my_experiment\" has two runs, we expect the created cache to have two entries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cache_from_archive = model.create_cache_from_archive()\nprint(cache_from_archive)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The displacement curves of both results can be compared.They are identical for this displacement-imposed loading:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_curves(\n    [\n        result.get_curve((\"dplt_grid\", \"dplt\")),\n        result_1.get_curve((\"dplt_grid\", \"dplt\")),\n    ],\n    labels=[\"result\", \"result 1\"],\n    save=False,\n    show=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However the resistance force at beam end are different:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variable_names = [\"height\", \"reaction_forces\"]\ndf = concat(\n    [\n        DataFrame([result.get_numeric_scalars(variable_names=variable_names)]),\n        DataFrame([result_1.get_numeric_scalars(variable_names=variable_names)]),\n    ],\n    ignore_index=True,\n)\nplot = BarPlot(Dataset.from_dataframe(df))\nplot.title = \"Comparison of model result with data\"\nplot.font_size = 20\nplot.labels = [\"height 50mm\", \"height 60mm\"]\nfig = plot.execute(save=False, show=True, file_format=\"html\")[0]\nfig"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}