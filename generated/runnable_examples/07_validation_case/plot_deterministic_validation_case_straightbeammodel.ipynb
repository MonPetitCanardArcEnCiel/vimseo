{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nDeterministic validation case on the bending test analytical beam model against an analytical solution\n======================================================================================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport logging\n\nfrom vimseo import EXAMPLE_RUNS_DIR_NAME\nfrom vimseo.api import activate_logger\nfrom vimseo.api import create_model\nfrom vimseo.core.model_settings import IntegratedModelSettings\nfrom vimseo.problems.beam_analytic.reference_dataset_builder import (\n    bending_test_analytical_reference_dataset,\n)\nfrom vimseo.tools.validation_case.validation_case import DeterministicValidationCase\nfrom vimseo.tools.validation_case.validation_case import (\n    DeterministicValidationCaseInputs,\n)\nfrom vimseo.tools.validation_case.validation_case import (\n    DeterministicValidationCaseSettings,\n)\n\nactivate_logger(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First a model is created:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"BendingTestAnalytical\"\nload_case = \"Cantilever\"\nmodel = create_model(\n    model_name,\n    load_case,\n    model_options=IntegratedModelSettings(\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/archive/validation_case\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/validation_case\",\n        cache_file_path=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/caches/validation_case/{model_name}_{load_case}.hdf\",\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The samples are set from synthetic reference data already generated for this model,\nto which a bias is added to obtain non-zero error metrics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reference_data = bending_test_analytical_reference_dataset(shift=10.0)[\"Cantilever\"]\nprint(\"The measured data: \", reference_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, the validation case tool is created and executed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_tool = DeterministicValidationCase()\nresults = validation_tool.execute(\n    inputs=DeterministicValidationCaseInputs(\n        model=model,\n        reference_data=reference_data,\n    ),\n    settings=DeterministicValidationCaseSettings(\n        metric_names=[\n            \"RelativeErrorMetric\",\n            \"AbsoluteErrorMetric\",\n        ],\n        output_names=[\"reaction_forces\"],\n    ),\n)\nvalidation_tool.save_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Post-processing the results.\n\nThe validation result contains:\n\n  - a dictionary mapping the error metric names,\n    output names and the statistical metric values (by default, the mean is used).\n  - a dataset containing the element-wise error metrics, together with the\n  - the simulated data, reference data and the input samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"The validation result: \", validation_tool.result)\nprint(validation_tool.result.element_wise_metrics)\nprint(validation_tool.result.integrated_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation results can be visualized as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figs = validation_tool.plot_results(\n    validation_tool.result,\n    \"RelativeErrorMetric\",\n    \"reaction_forces\",\n    save=False,\n    show=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a parallel coordinates plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figs[\"parallel_coordinates\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "an error scatter matrix:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figs[\"error_scatter_matrix\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a predict-versus-true plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figs[\"predict_vs_true\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a bar plot of the integrated metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figs[\"integrated_metric_bars\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}