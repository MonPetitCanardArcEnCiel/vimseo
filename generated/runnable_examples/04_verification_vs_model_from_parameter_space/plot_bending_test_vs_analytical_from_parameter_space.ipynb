{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUsage of the model-to-model solution verification tool on a Finite-Element (Abaqus) model\n=========================================================================================\n\nCheck an Abaqus cantilever beam model versus an analytical model with the\n'CodeVerificationAgainstModelFromParameterSpace' tool using as input a parameter\nspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport logging\n\nfrom gemseo.utils.directory_creator import DirectoryNamingMethod\n\nfrom vimseo import EXAMPLE_RUNS_DIR_NAME\nfrom vimseo.api import activate_logger\nfrom vimseo.api import create_model\nfrom vimseo.core.model_settings import IntegratedModelSettings\nfrom vimseo.tools.space.space_tool import SpaceTool\nfrom vimseo.tools.verification.verification_vs_model_from_parameter_space import (\n    CodeVerificationAgainstModelFromParameterSpace,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define the logger level:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "activate_logger(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, the two models are compared over an input parameter space.\nSo we need to generate a space of parameters.\nIt is obtained using the ``SpaceTool`` and choosing the ``FromModelCenterAndCov``\nbuilder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "space_tool = SpaceTool(working_directory=\"SpaceTool_results\")\nspace_tool.execute(\n    distribution_name=\"OTTriangularDistribution\",\n    space_builder_name=\"FromMinAndMax\",\n    minimum_values={\n        \"length\": 200.0,\n        \"height\": 5.0,\n        \"imposed_dplt\": 0.0,\n        \"relative_dplt_location\": 0.1,\n    },\n    maximum_values={\n        \"length\": 1000.0,\n        \"height\": 50.0,\n        \"imposed_dplt\": 20.0,\n        \"relative_dplt_location\": 1.0,\n    },\n)\nprint(space_tool.parameter_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then let's create the model to verify:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"BendingTestAnalytical\"\nload_case = \"Cantilever\"\nmodel = create_model(\n    model_name,\n    load_case,\n    model_options=IntegratedModelSettings(\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/archive/verification_vs_model\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/verification_vs_model\",\n    ),\n)\nmodel.cache = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And the reference model:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_2 = create_model(\n    \"BendingTestAnalytical\",\n    load_case,\n    model_options=IntegratedModelSettings(\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/archive/verification_vs_model_2nd_model\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/verification_vs_model_2nd_model\",\n    ),\n)\nmodel_2.cache = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All inputs to the verification are now available.\nWe create the verification tool we are interested in.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator = CodeVerificationAgainstModelFromParameterSpace(\n    directory_naming_method=DirectoryNamingMethod.NUMBERED,\n    working_directory=\"CodeVerificationAgainstModelFromParameterSpace_results\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The options can be modified.\nAlternatively, options can be passed as keyword arguments to\n``CodeVerificationAgainstModelFromParameterSpace()`` constructor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.options[\"metric_names\"] = [\n    \"SquaredErrorMetric\",\n    \"RelativeErrorMetric\",\n    \"AbsoluteErrorMetric\",\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The verification is now executed on 50 samples using by default an optimised Latin\nhypercube algorithm.\nNote that the verification is here restrained to the output variable\n``reaction_forces``, and that a description of the verification can be provided.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.execute(\n    model=model,\n    reference_model=model_2,\n    parameter_space=space_tool.result.parameter_space,\n    n_samples=5,\n    output_names=[\"reaction_forces\", \"maximum_dplt\", \"location_max_dplt\"],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the error metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.result.integrated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And saved on disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.save_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The saved results can be loaded in a dedicated dashboard to be explored.\nThe dashboard is opened by typing ``dashboard_verification`` in a terminal,\nand selecting the tab ``Comparison case``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results can also be plotted from the Python API.\nIt shows the scatter matrix of the inputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figures = verificator.plot_results(\n    verificator.result,\n    \"RelativeErrorMetric\",\n    \"reaction_forces\",\n    save=False,\n    show=True,\n    directory_path=verificator.working_directory,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and an histogram of the errors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figures[\"error_metric_histogram\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}