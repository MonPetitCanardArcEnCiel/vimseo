{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nUsage of the model-versus-data code verification tool on a Finite-Element (Abaqus) model\n=========================================================================================\n\nCheck an Abaqus cantilever beam model versus a reference dataset with the\n'CodeVerificationAgainstData' tool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport logging\n\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.utils.directory_creator import DirectoryNamingMethod\n\nfrom vimseo import EXAMPLE_RUNS_DIR_NAME\nfrom vimseo.api import activate_logger\nfrom vimseo.api import create_model\nfrom vimseo.core.model_settings import IntegratedModelSettings\nfrom vimseo.tools.verification.verification_vs_data import CodeVerificationAgainstData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first define the logger level:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "activate_logger(level=logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create the model to verify:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"BendingTestAnalytical\"\nload_case = \"Cantilever\"\nmodel = create_model(\n    model_name,\n    load_case,\n    model_options=IntegratedModelSettings(\n        directory_archive_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/archive/verification_vs_data\",\n        directory_scratch_root=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/scratch/verification_vs_data\",\n        cache_file_path=f\"../../../{EXAMPLE_RUNS_DIR_NAME}/caches/verification_vs_data/{model_name}_{load_case}_cache.hdf\",\n    ),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also need a reference dataset.\nHere we do it programmatically, but we can also create it from a csv file:\n``\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reference_data = IODataset().from_array(\n    data=[[10.0, 10.0, -4.0, -12.0], [15.0, 10.0, -6.0, -40.0]],\n    variable_names=[\"height\", \"width\", \"maximum_dplt\", \"reaction_forces\"],\n    variable_names_to_group_names={\n        \"height\": \"inputs\",\n        \"width\": \"inputs\",\n        \"maximum_dplt\": \"outputs\",\n        \"reaction_forces\": \"outputs\",\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All inputs to the verification are now available.\nWe create the verification tool we are interested in.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator = CodeVerificationAgainstData(\n    directory_naming_method=DirectoryNamingMethod.NUMBERED,\n    working_directory=\"CodeVerificationAgainstData_results\",\n)\n\n# The options can be modified.\n# Alternatively, options can be passed as keyword arguments to\n# ``CodeVerificationAgainstModelFromParameterSpace()`` constructor.\nverificator.options[\"metric_names\"] = [\n    \"SquaredErrorMetric\",\n    \"RelativeErrorMetric\",\n    \"AbsoluteErrorMetric\",\n]\n\nverificator.execute(\n    model=model,\n    reference_data=reference_data,\n    output_names=[\"maximum_dplt\", \"reaction_forces\"],\n    description={\n        \"title\": \"Verification of a cantilever analytic beam for a variation of beam height.\",\n        \"element_wise\": [\"Small height value\", \"High height value\"],\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The result contains the error metrics:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.result.integrated_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And saved on disk, together with its metadata:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "verificator.save_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The saved results can be loaded in a dedicated dashboard to be explored.\nThe dashboard is opened by typing ``dashboard_verification`` in a terminal,\nand selecting the tab ``Comparison case``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results can also be plotted from the Python API.\nIt shows the scatter matrix of the inputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figures = verificator.plot_results(\n    verificator.result,\n    \"RelativeErrorMetric\",\n    \"reaction_forces\",\n    save=False,\n    show=True,\n    directory_path=verificator.working_directory,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and an histogram of the errors:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "figures[\"error_metric_histogram\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}